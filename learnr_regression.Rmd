---
title: "Linear models with continuous explanatory variables"
output: learnr::tutorial
runtime: shiny_prerendered
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
library(learnr)
library(mosaic)
library(ggpubr)
caterpillers <- read.csv("www/regression.csv")
knitr::opts_chunk$set(echo = FALSE)
```

<style type="text/css">
span.boxed {
  border:5px solid gray;
  border-radius:10px;
  padding: 5px;
}
span.invboxed {
  border:5px solid gray;
  padding: 5px;
  border-radius:10px;
  color: white;
}
table, td, th
{
border:0px;
}
</style>




## What are linear models?

You will all be familiar with linear models already from school even if
you don't realise. The simplest form of linear model is the one that
describes a straight line:

$$y = a + bx$$

where

-   $y$ = dependent (or response) variable
-   $x$ = independent (or explanatory)
-   $b$ = parameter to represent the slope of the relationship between
    the $x$ and $y$ variables. $bx$ is the same as $b * x$
-   $a$ = intercept (if applicable)

### But what specifically are linear models?

Most of the common statistical models (t-test, correlation, regression,
ANOVA, chi-square, etc.) are special cases of linear models or a very
close approximation. If we think about this from a statistical modelling
perspective, we have the following:

$$\text{Response variable } (y)=\text{Model}(\text{some form of }a + bx)+error$$

The model includes an explanatory variable ($x$) and a series of
parameters (e.g. $a$ and $b$) relating the explanatory variable to the
response variable ($y$). The important thing about a linear model is
that we can add additional variables to those that explain the variation
in the response variable ($y$) and go beyond just describing a straight
line. The $error$ in the equation above is the variation in your data
that the model cannot explain. It doesn't matter how good your
experiment or field study is, there will always be some noise in your
data.

Linear models have a lot of flexibility because we can include
explanatory variables which are continuous or categorical or a
combination of both in describing the variability in response variable.
This means that linear models can also describe relationships which do
not appear to be straight lines. Linear models can describe a
curvilinear response or, when multiple explanatory variables are
included, relationships that do not appear as curvilinear or straight.

The term "linear" can, therefore, be used in two different ways. The
first, which you are familiar with already, is to describe a straight
line. The second is more complicated and is a more accurate description.
A linear model is one where any value of interest or response variable
($y$) is described by a linear combination of independent explanatory
variables ($x$).

## Continuous explanatory variables
### When might continuous variables occur?
You will often have data in the form of a continuous response variable
and a continuous explanatory variables. Examples might include:

|   Response variable    | Explanatory variable  |
|:----------------------:|:---------------------:|
|   Species diversity    |    Area of habitat    |
|    Bacterial growth    |    Antibiotic dose    |
| Invertebrate pest loss | pesticide application |

The first example is from a 'natural experiment' or survey, the others
are from designed experiments in the laboratory or the field. The
principles of the analysis are the same each time. This tutorial shows
you how to visualise data from these types of studies, undertake the
appropriate analysis with a linear model, and learn how to interpret the
analysis.

### What do we mean by "cause and effect"?
In the above examples it should be relatively obvious that some variables can 
have an impact on others, but the reverse doesn't make biological sense. So if
you have a laboratory experiment, you as the experimenter decide on e.g. 7
different antibiotic doses to use, and that determines the relevant growth rate.
The reverse, that the growth rate of bacteria determines how big a dose you
decided to put into the Petri dish before you began the experiment, makes no
logical sense. You'll see the terms "independent" and "dependent" also used to
represent "cause" and "effect" respectively. Note that you do not need to know
the exact mode of causation to identify your variables. The area of habitat
available is know to affect species diversity, but several competing theories
have been proposed to explain the mechanism.

### What if I'm unsure which variable is "cause" and which "effect"?
Sometimes, typically in 'natural' surveys, it may not be
obvious which is the causal (explanatory) and which is the affected (response)
variable. For example,
the numbers of predators and prey may be related at different sites, but
they also change over time. You can still analyse these data using
linear models, but be careful not to infer cause and effect. Correlation
may be more appropriate. Correlation simply quantifies the strength of the
relationship between two variables, but does not infer cause and effect. For 
example:

```{r echo = FALSE}
set.seed(123)
a <- (1:30) + abs(rnorm(10)) * 2
b1 <- a * 5 + abs(rnorm(30)) * 50
b2 <- a * 5 + abs(rnorm(30)) * 165
corr_dat <- data.frame(a = c(a, a), b = c(b1, b2), correl = c(rep("High", 30), rep("Low", 30)))

ggplot(corr_dat, aes(x = a, y = b)) +
  geom_point() +
  stat_cor(label.x.npc = "left", label.y.npc = "top", r.accuracy = 0.001, p.accuracy = 0.001) +
  xlab("Variable A") +
  ylab("Variable B") +
  facet_wrap(~ correl) +
  theme_classic()
```

This plot shows the scatter between two variables, A and B, as measured by the
correlation coefficient $R$, sometimes shown as $r$. The correlation coefficent
goes from -1 (perfect negative correlation), to 0 (no correlation) through to 1
(perfect positive correlation). In the above graph the scatter of points
**increases** from left to right in both graphs so we have **positive** 
correlation coefficients displayed. However, the correlation coefficient is much
higher in the graph on the left, as the data are less noisy.

### Linear models quantify cause and effect
In most of your studies you are likely to have a good understanding of which
are your explanatory and response variables. This is particularly going to arise
when you develop **hypothesis-driven** research questions, both for laboratory
and field research. This approach as allowed all areas of science to develop
most rapidly, and if you have good, well-defined research questions you will
find it easier to develop you data collection / field surveys to a high standard
and know how to analyse and interpret your findings.

Linear models assume the basic structure of:

&nbsp;

<center>
<h2><strong><span class="boxed">Response</span> ~ <span class="boxed">Explanatory(s)</span> + <span class="boxed">Noise</span></strong> 
</h2></center>

&nbsp;

There are two components here:

* **Signal** = impact of your explanatory variable(s) on your response
* **Noise**  = unknown variation in your data

All linear models measure the signal to noise ratio. The **lower** you can make
the noise the more likely you will be able to detect the effects of your
explanatory variables. You can reduce the noise by changes in your experimental/
field survey design, and also what assumptions you make about the characteristics of the
unknown noise in the data. This will improve the inferences you can make from
your laboratory experiments and field surveys.


## Data exploration
### The experimental data

Here are some data from a laboratory experiment to investigate the relationship
between caterpiller growth rates and the amount of tannins in their
diet. Tannins are produced by some plants to reduce attack by insects. They are
what gives black tea its slightly bitter taste. The caterpillers were fed diets
that contained different amounts of tannins (mg), and the caterpiller growth 
rate (mg/day) measured.

Before you continue, answer the following question:

```{r which_is_response, echo=FALSE}
question("Which of the following statements is true?",
         answer("growth rate is the independent variable and tannin is dependent"),
         answer("the response is the growth rate and tannin is explanatory", correct = TRUE),
         answer("either variable could be treated as response or explanatory")
)
```

Note that in some books response variables will be called 'dependent',
whilst explanatory variables will be called 'independent', so it is
useful to be aware of both sets of terms.


The data for this have been stored in an dataframe called
`caterpillers`. It is useful to plot these before proceeding.

```{r x_or_y, echo=FALSE}
question("What variables go on the x (horizontal) and y (vertical) axes?",
         answer("growth rate on x axis and tannins on y axis",
                message = "By convention, explanatory variables on x axis"),
         answer("tannins on x axis and growth rate on y axis", correct = TRUE)
)
```

Write the R code required calculate the means (averages) of the values
in the `caterpillers` dataframe. Click the `hint` button if you get stuck.

```{r basic_stats, exercise=TRUE, exercise.completion=FALSE}
  
```

::: {#basic_stats-hint}
**Hint** Use the `ls()` function to see what is in your workspace, use
the `summary()` function to get some basic statistics, that includes the averages.
:::

Now produce a scatterplot of the caterpiller growth rate (y-axis)
against the amount of tannins in the diet (x-axis). Use `gf_point()`
function to display your data as points in an x-y scatterplot. The `gf_`
or "graph formula" functions are more flexible than base graphics, and
tend to follow a "formula" syntax.

```{r scatter_plot, exercise=TRUE, exercise.completion=FALSE}

```

```{r scatter_plot-solution}
gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_theme(theme_classic())
```

Imagine fitting a straight line to this scatter of points in your plot,
then answer the question below:

```{r gradient_and_slope, echo=FALSE}
question("Which statements below are true?",
         answer("Catterpiller growth rate is maximum when tannins are high"),
         answer("The intercept of the fitted line is negative"),
         answer("The gradient of the fitted line is negative", correct = TRUE),
         answer("Tannins inhibit caterpiller growth rate", correct = TRUE),
         answer("The intercept for the fitted line is positive", correct = TRUE)
)
```

## Creating a linear model
A linear model is used to test your null hypothesis $H0$  (it does not formally
test your alternative hypothesis $HA$):

* $H0$ = null hypothesis = probability that your explanatory variable has **no
significant effect** on your response variable
* $HA$ = alternative hypothesis = probability that your explanatory variable
affects your response variable.

This is a bit confusing, because you're probably more interested in $HA$ than in
$H0$ but it does not matter. Your linear model will produce a **p-value** which is
the probability of $H0$ being true. If your p-value is very low (less than 0.05)
then we "reject" the null hypothesis. We thus "implicitly" accept the alternate
hypothesis, even though we don't formally test it.

For our caterpiller growth vs tannins dataset, our null hypothesis $H0$ would be
"Tannins have no affect on caterpiller growth rate". Our linear model will allow
us to accept this hypothesis if the probability of the hypothesis being true is
greater than 5% (p > 0.05), or reject it if the probability of it being true is
less than 5% (p < 0.05). The 0.05 cutoff is a scientific convention.

The `lm()` function is used to create a linear model, with the standard
syntax of

`lm(response_variable ~ explanatory_variable, data=dataframe_name)`

It is usually best to store the results of your linear model in a new R
object, so that you can summarise, display or use the results of your
linear model:

`lm_results <- lm(response_variable ~ explanatory_variable, data=dataframe_name)`
`summary(lm_results)`

In the R coding space below, create a linear model for your
`caterpiller` data, making sure that you specify the response and
explanatory variables the right way round. Store the results in an R
objected called `caterpiller_lm` then use the `summary()` function to
display the results of the analysis.

```{r lm_calc, exercise=TRUE, exercise.completion=FALSE}

```

```{r lm_calc-solution}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
summary(caterpiller_lm)
```

You should see an output summary, with quite a lot of information in it,
but there are only a small number of components that are critical, so
don't be daunted! The next section shows you how to understand the table
output by the `summary()` command.

## How to interpret lm summary

If your linear model is correct, you should see something like:

```{r summary_table, echo=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
summary(caterpiller_lm)
```

The first line simply shows you how you called the `lm()` function, and
it is worth checking this, just to ensure you used it in the way you
think you did:

`lm(formula = growth ~ tannin, data = caterpillers)`

The next couple of rows summarise information about the **residuals**.
We will describe how these are calculated in more detail shortly. The
coefficients table is important and has 5 columns:

|             | Estimate | Std. Error | t value | Pr(\>`|`t`|`)   |
|-------------|----------|------------|---------|-----------------|
| (Intercept) | 11.7556  | 1.0408     | 11.295  | 9.54e-06 \*\*\* |
| tannin      | -1.2167  | 0.2186     | -5.565  | 0.000846 \*\*\* |

### first column (no heading title)

Gives the names of the parameters estimated by your linear model. Here
it is two parameters:

-   (Intercept): This is where your fitted line intersects the y-axis
    when x is zero
-   tannin: This is the gradient of your fitted line. The rate of change
    in growth rate with tannin.

### second column (headed `Estimate`)

Gives the values calculated by the linear model for your two parameters:

-   `r round(caterpiller_lm$coefficients[1],2)` for the intercept
-   `r round(caterpiller_lm$coefficients[2],2)` for the gradient. This
    is **negative** as growth rate is low with high tannin, i.e. the
    line goes down from left to right.

### third column (headed `Std. Error`)

Provides the standard errors for your estimates.

### fourth column `t value` gives the estimated statistic from the **t distribution**

which is a statistical distribution (developed to improve the brewing of
Guinness stout!).

### Fifth and final column, headed `Pr(>|t|)`

This indicates "how likely you are to have obtained these values from a
t-distribution according to the null hypothesis $H0$". That's very confusing!
A simpler interpretation is "what is the probability of obtaining the
estimated intercept and gradient if there was **no relationship** between
caterpiller growth rate and tannin". Basically, the **lower** these
**p-values** the **more important** the gradient and intercept.

If the p-values are very low, the way R displays them is confusing. You
can see:

-   (Intercept) 9.54e-06
-   tannin 0.000846

which is the same as

-   (Intercept) 0.00000954
-   tannin 0.000846

Both these p-values are very low, **much lower than the conventional
cutoff of 0.05**, so we can say that the results are highly significant.
This is also indicated by the three asterisks after the numbers.
**Important** When p-values are very low, less than 0.001, it is best to
report them as *p \< 0.001* rather than write out the full set of
decimal places.

```{r understanding_summary, echo=FALSE}
question("Which of these statements is true?",
         answer("The estimate for Intercept is the value on the y-axis when the x-axis is zero", correct=TRUE),
         answer("The p-value is the probability that the parameter is zero",
                message = "Very low p-values indicate that it is unlikely that the parameter is zero", correct=TRUE),
         answer("The intercept will always be significant", message = "Sometimes the intercept might be
                non-significant, so that the fitted line goes through the zero on the x and y axes. You can
                omit the intercept by fitting the model as `lm(response ~ 1 - explanatory)`"),
         answer("A p-value shown as 2.34e-05 is the same as 0.0000234", correct=TRUE),
         allow_retry = TRUE
)
```

The final three rows give:

-   The residual standard error (don't worry about this for now)
-   R-squared values (see below)
-   F-statistic, degrees of freedom, and p-value (see below)

**The R-squared value** goes from zero to 1.0, and can be interpreted as a
percentage. It indicates what proportion (or percentage) of your y
(response) value can be explained by your x-value (explanatory). Two
R-squared values are given, and always use the one headed
`Adjusted R-squared`. You have a value of 0.7893 which means that almost
79% of the variation in caterpiller growth can be explained by variation
in tannin.

**F-statistics** can be produced by all linear models. Here, this is an overall
one, for the (single) explanatory variable of tannin. As you only have
one explanatory variable, your overall F-statistic has the same p-value
as shown against tannin earlier on, of 0.000846. The larger the
F-statistic, the more significant is the overall linear model.

**Degrees of freedom (DF)** Two numbers are displayed for DF, 1 and 7. The first value,
1, relates to the number of parameters being estimated, not counting the Intercept,
which here is the slope of the line. The second is the number of observations (9)
minus the number of parameters estimated (two, for the slope and intercept).

**p-value** This is the overall probability of your null hypothesis $H0$ of no
effect of tannins on growth being true. You can see that it is very low, so we
can reject the null hypothesis. You only have one explanatory variable, `tannin`
in this linear model, so this p-value is identical to the one shown against the
`tannin` row higher up in the output. Sometimes you will have multiple
explanatory variables, and so the overall model p-value will differ.

## Model predictions

Our linear model indicated that the relationship between tannins and caterpiller growth rate is
highly significant therefore it would be usefule to display the predicted line
from the linear model onto your scatter of points.
Remember that your fitted line will have the same intercept and gradient
as shown in your summary, with a negative gradient (slope) reflecting
the decline from left to right. This is easiest to plot using the
`gf_point()` function that you used earlier. You combine this with the
`gl_lm()` function to display the fitted line. To do this, use what is
known as a "pipe" operated in R, using the notation `%>%` which can be
interpreted as the English word "then". So, in English you

-   Create a scatterplot, with growth as response, tannin as explanatory
    **then**
-   add the fitted line

which in R code is:

```{r show_fitted_plot, echo=TRUE, eval=FALSE}
gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_lm() %>% 
  gf_theme(theme_classic())
```

Notice the symbol `%>%` at the end of the first line, which you should
read as "then". Try out these commands below. What happens if you run:

`gf_lm(growth ~ tannin, data=caterpillers)`

What is the effect of adding the argument `interval = "confidence"` to the `gf_lm()` function?

```{r plot_data, exercise=TRUE, exercise.completion=FALSE}
  
```
```{r plot_data-solution}
# Scatterplot with fitted line
gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_lm() %>% 
  gf_theme(theme_classic())

# gf_lm simply produces the line
gf_lm(growth ~ tannin, data=caterpillers) %>% 
  gf_theme(theme_classic())

# You can add standard error bands to the fitted line
gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_lm(interval = "confidence") %>% 
  gf_theme(theme_classic())

```


### Obtaining the fitted values for input x-values

The fitted or "predicted" values are those along your straight line, which you have just plotted. For
the 9 values of tannin, these are stored in the model output, under the
`fitted.values` option which you can access by typing
`caterpiller_lm$fitted.values` after you have created your linear model.
Try it now, and it displays the nine fitted values for tannin levels 1
to 9. Compare it with the graph you created earlier.

```{r lm_fitted, exercise=TRUE, exercise.completion=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)

```
```{r lm_fitted-solution}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
model_fitted_values <- caterpiller_lm$fitted.values
model_fitted_values
```


### Fitted for new input x-values

What if you wanted to know the predicted growth rate for a new level of
tannin, that was not in your original dataset. For example, tannin at
4.36 ? There are two ways of doing this:

1.  Use R as a calculator, and take the values for intercept and slope
    than you displayed earlier, to work out the value. i.e.
    `11.76 - (1.22 * 4.36)`. This method can be difficult if you have
    several explanatory variables, or a complex linear model.
2.  Create a function, using `makeFun`, derived from your model, to
    predict the results. This is more flexible, especially with complex
    models.

In the next code-box we create our own command function called `predictor` into
which we can enter a single tannin value (`4.36`) or the tannin values 1 to 10
via the shortcut `1:10`:

```{r lm_makeFun, echo=TRUE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
predictor <- makeFun(caterpiller_lm)
predictor(4.36)
predictor(1:10)
```

Notice above where we put a range of values into our `predictor`
function that where we had a tannin value of 10, we obtained negative
growth. Be very careful about making predictions beyond the range of
your input data as they may not be biologically meaningful.

Experiment below with a predictor function created by `makeFun`. You can
call the function you create whatever you want (you don't need to call
it `predictor`).

```{r lm_makeFun2, exercise=TRUE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
```
```{r lm_makeFun2-solution}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)

# Create your prediction function. Let's call it growth_predictor
growth_predictor <- makeFun(caterpiller_lm)

# Now predict the growth rate at e.g. tannin 7.12 mg
growth_predictor(7.12)
```

## Checking model assumptions

### Model assumptions
All statistical tests have assumptions associated with them and we need to check whether our assumptions have been met so that we know we have undertaken the correct analysis and our subsequent inference from our statistical tests are valid. There are three key assumptions that you should always consider:

* Normally distributed errors
* Homogeneity of variance
* Independence of replicates

Most of these can be informally assessed by eye, or consideration of how you collected your data.

#### **Normally distributed errors**
We have already mentioned that your linear model has this structure:

&nbsp;

<center>
<h2><strong><span class="boxed">Response</span> ~ <span class="boxed">Explanatory(s)</span> + <span class="boxed">Noise</span></strong> 
</h2></center>

&nbsp;
 
but we have focussed entirely on the response and explanatory variables, we haven't talked about the errors. These are also called "residuals". They are the unknown "noise" from your model predictions as represented by the blue straight line, and the black data points of your observations:

```{r fitted_line_and_data, echo=FALSE, warning=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_lm() %>% 
  gf_theme(theme_classic())
```

These errors (residuals) might be clearer to see if highlighted in red; they are the difference between your fitted and observed values. Note that some of the errors are positive (above the blue fitted line) and some are negative (below the line), and the sum of these errors will be zero:

```{r fitted_line_and_data2, echo=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
fits <- caterpiller_lm$fitted.values

gf_point(growth ~ tannin, data=caterpillers) %>% 
  gf_lm() %>% 
  gf_segment(growth + fits ~ tannin + tannin, colour = "red") %>% 
  gf_theme(theme_classic())
```

Not all your observations fall exactly on the line, as sometimes they are slightly above, and sometimes below. You can see that there is “noise” around the fitted line. The residual for each observation or data point is the difference between the predicted values of y (the line) and the observed value of y. Even if you do an amazingly good experiment, it is very unlikely that they will all fall exactly on the line. You will always have residuals. In some cases, extra explanatory variables can be added to the regression, which will explain more of the variation.

```{r residuals_basics, echo=FALSE}
question("Which of the following statements is true?",
         answer("Residuals are also known as errors or noise",
                correct = TRUE, message = "Residuals, noise and errors are all valid terms"),
         answer("If you increase the replication the residuals will disappear",
                message = "You will still have unexplained variation even with more replicates"),
         answer("You need to do the experiment again",
                message = "Not necessarily; the biological system might be naturally noisy"),
         answer("Check residuals in case any outliers were due to experimental error",
                correct=TRUE),
         answer("Sometimes including extra explanatory variables will reduce the residuals",
                correct=TRUE),
         allow_retry = TRUE
)
```

You wrote your model as

`lm(growth ~ tannin, data=caterpillers)`

which is the equivalent of

$$Response = Explanatory\space variable(s)$$ However the above equation
is actually incomplete. In full the equation is:

$$Response = Explanatory\space variable(s) + Error$$ where $Error$
represents your residuals. You might also see it represented as:

$$Y = X + \epsilon$$ where the $Y$ is the response, $X$ is one (or more)
explanatory variable(s), and the Greek letter epsilon $\epsilon$
represents the residual error.

### Looking at your residuals

It is easy to view the residuals from your model using the `residuals()`
function; try it here to look at the numbers from your `caterpiller_lm`
linear model. Try it out now:

```{r lm_residuals, exercise=TRUE, exercise.completion=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)

```

::: {#lm_residuals-hint}
Simply use the `residuals()` function with your model output
`caterpiller_lm` as its parameter to display the positive and negative
residual values.
:::

If you had a big experiment, with e.g. 30 individual observations, you
could use the `gf_histogram()` function to display a frequency histogram
of your residuals, and check that they look approximately normal
(bell-shaped curve). However, your experiment only has 9 observations,
so it is difficult to assess this histogram:

```{r histogram_residuals}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)
gf_histogram(~ residuals(caterpiller_lm)) %>% 
  gf_dens()
```

There are so few points, that the bars of the histogram do not show any
obvious pattern, although adding a smoothed density curve via
`gf_dens()` implies that we have a bell-shaped curve. However we need a
more sophisticated method of checking our residuals.

### QQ plots

Quantile-quantile plots (QQ plots) are a very simple way of checking
your model residuals, and are useful, as here, when we have small
datasets. So what is a quantile? Imagine a typical bell-shaped curve,
from a normal distribution. Let us create a set of random data from a
normal distribution, with a mean of zero, and display its frequency
histogram:

```{r normal_random}
my_data <- rnorm(200) # Creates 50 data points from a random distribution
mean(my_data)        # Check the average is zero
gf_histogram(~ my_data) 
```

You can see that it is centred around zero, and roughly bell-shaped. All
a QQ plot does is sort your residual data from lowest to highest, and
sees how well it matches with what you would expect in theory. Let's
look at this with some more randomly generated data, against a dotted
line showing what values would have been obtained if there was a perfect
match:

```{r normal_random_with_qqplot}
my_data <- rnorm(200) # Creates 50 data points from a random distribution
gf_qq(~ my_data)  %>% # Your 50 data points
  gf_qqline()
```

The match between the random data (points) and that expected in theory
(dots) is of course excellent. But what about your data from your
caterpillers when you only have 9 data points? Try it now:

```{r caterpiller_qqplot, exercise=TRUE, exercise.completion=FALSE}
caterpiller_lm <- lm(growth ~ tannin, data=caterpillers)

```

```{r caterpiller_qqplot-solution}
# Store the residuals in a new variable called model_error, then use the
# gf_qq() and gf_qqline() functions to display the QQ plots
model_error <- residuals(caterpiller_lm)
gf_qq( ~ model_error) %>% 
  gf_qqline()
```

As you can see, your QQ plot points match very closely to the
theoretical ideal of a straight line suggesting that your linear model
is robust.

## How to write up your analyses

Your reports for most of your work, especially in Stage 3, are likely to have the
following format:

* Abstract
* Introduction
* Methods
* Results
* Discussion
* References

Look in the **NES2303 Course Handbook** on Canvas for some detailed advice on
how to write reports especially if you find scientific writing hard. Here we
focus on the Methods and Results

### Methods
This will have two or more sections, the first explaining the laboratry 
procedures or field survey methods used. For a field survey, you might want to
begin your Methods with a separate section to provide a site description.
Whether you do a laboratoy or field-based study, the last section **must**
contain an explanation of the data analyses used and software. For example:

"All laboratory data were initially recorded in Excel spreadsheets for each
experiment. These were saved as .CSV files, and analysed in R 4.3 using linear
models, to determine whether caterpiller diet had any impact on caterpiller
growth rate. Linear model assumptions of normally-distributed errors were
checked through the creation of quantile-quantile residual plots."

### Results
Results must **always** start with a paragraph of text. Do not start with a
table, graph or similar. The text should cross-reference your Tables / Figures.
Present your result in a factual, impartial way. For example: "There was a
significant negative relationship between growth and tannin (Figure 1; t=-5.565,
P \< 0.001) with 78.9% of the variability in growth explained by tannin (Table 1)."

Ensure that Figure 1 (your scatterplot with fitted line) has a caption and is
nicely formatted. You will **not** be able to use the output from your
`summary(caterpiller_lm)` function as a direct copy-and-paste to create Table 1.
**Always** retype the R output so it is properly formatted, with a sensible 
number of decimal places. If reporting F-ratios, remember to include the 
degrees of freedom as subscripts, e.g. "The overall linear model for tannins and
growth was signficant (F<sub>1,7</sub>=30.97, P \< 0.001)"

### Common bad practice when reporting Methods and Results
Please avoid these common mistakes as you will lose marks!

* Not mentioning data analyses in your Methods
* Including Tables and Figures in the report, but not cross-referencing them
* A Results section that consists entirely of Tables/Figures but no text
* Too many decimal places from R output copied unedited
* P-values presented as 3.4124e-11 rather than p<0.001
* "The regression proved that tannins affect growth" **Note** Never use
    'proved' in reports etc.

## Summary and further reading
### Key points learnt
Hopefully you now have a good understanding of

* Response and explanatory variables, and how to identify them
* Basic concepts of a linear model
* How to create a linear model, interpret its outputs, and check the assumptions
that underly the model
* Good practice for reporting your data science

### Reading
There is a lot of excellent material available at the Library and online.
Particularly recommended are:

* Experimental Design and Data Analysis for Biologists by Quinn and Keough Chapter 5
* Getting Started with R Chapter 5
* Zuur et al A protocol for data exploration to avoid common statistical problems (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210X.2009.00001.x)  This is
published in the main journal of the British Ecological Society - it is quite challenging in places
but will give you a really good understanding.